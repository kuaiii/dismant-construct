# è®­ç»ƒè®¾ç½®æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•è®¾ç½®è®­ç»ƒç¯å¢ƒå¹¶å¼€å§‹è®­ç»ƒã€‚

## âš ï¸ é‡è¦æç¤º

å½“å‰ä»£ç æ¡†æ¶ä¸­çš„ LLM åŠ è½½éƒ¨åˆ†ï¼ˆ`src/model/fusion_llm.py` ä¸­çš„ `_load_llm` å’Œ `_apply_lora` æ–¹æ³•ï¼‰æ˜¯**å ä½ç¬¦å®ç°**ï¼Œéœ€è¦æ ¹æ®ä½ çš„å®é™…éœ€æ±‚è¿›è¡Œå®ç°ã€‚

## ğŸ”§ å‰ç½®è¦æ±‚

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. å‡†å¤‡ LLM æ¨¡å‹

ä½ éœ€è¦é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒçš„ LLM æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
- `meta-llama/Meta-Llama-3-8B`
- `Qwen/Qwen2-7B`
- `THUDM/chatglm3-6b`
- `mistralai/Mistral-7B-v0.1`

**æ³¨æ„**ï¼šéœ€è¦ HuggingFace è´¦å·å’Œè®¿é—®æƒé™æ‰èƒ½ä¸‹è½½æŸäº›æ¨¡å‹ã€‚

### 3. å®ç° LLM åŠ è½½

ç¼–è¾‘ `src/model/fusion_llm.py`ï¼Œå®ç° `_load_llm` å’Œ `_apply_lora` æ–¹æ³•ï¼š

```python
def _load_llm(self, device: str) -> None:
    """åŠ è½½é¢„è®­ç»ƒ LLM"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    self.llm = AutoModelForCausalLM.from_pretrained(
        self.config.llm_model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True  # å¦‚æœéœ€è¦
    )
    
    self.tokenizer = AutoTokenizer.from_pretrained(
        self.config.llm_model_name,
        trust_remote_code=True
    )
    
    # è®¾ç½® pad token
    if self.tokenizer.pad_token is None:
        self.tokenizer.pad_token = self.tokenizer.eos_token

def _apply_lora(self) -> None:
    """åº”ç”¨ LoRA é€‚é…å™¨"""
    from peft import LoraConfig, get_peft_model, TaskType
    
    lora_config = LoraConfig(
        r=self.config.lora_r,
        lora_alpha=self.config.lora_alpha,
        lora_dropout=self.config.lora_dropout,
        target_modules=self.config.lora_target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    self.llm = get_peft_model(self.llm, lora_config)
    self.llm.print_trainable_parameters()
```

## ğŸš€ å¼€å§‹è®­ç»ƒ

### æ­¥éª¤ 1: ç¡®ä¿æ•°æ®å·²ç”Ÿæˆ

```bash
# æ£€æŸ¥æ•°æ®æ–‡ä»¶
ls data/fine_tuning/combined/train.json
ls data/fine_tuning/combined/eval.json
```

### æ­¥éª¤ 2: è¿è¡Œè®­ç»ƒ

```bash
python scripts/train.py \
    --train_data data/fine_tuning/combined/train.json \
    --eval_data data/fine_tuning/combined/eval.json \
    --output_dir outputs/mixed_model \
    --phase 1 \
    --epochs 3
```

## ğŸ” å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### é”™è¯¯ 1: `optimizer got an empty parameter list`

**åŸå› **: æ¨¡å‹æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œé€šå¸¸æ˜¯å› ä¸ºï¼š
1. æ¨¡å‹æ²¡æœ‰è°ƒç”¨ `initialize()`
2. `_load_llm` æ–¹æ³•æœªå®ç°æˆ–æŠ›å‡ºå¼‚å¸¸

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®ä¿åœ¨è®­ç»ƒè„šæœ¬ä¸­è°ƒç”¨äº† `model.initialize(device)`
2. å®ç° `_load_llm` å’Œ `_apply_lora` æ–¹æ³•
3. æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½

### é”™è¯¯ 2: `CUDA out of memory`

**åŸå› **: GPU å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å° batch size: `--batch_size 2` æˆ– `--batch_size 1`
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: å¢åŠ  `gradient_accumulation_steps`
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–ä½¿ç”¨é‡åŒ–

### é”™è¯¯ 3: `Model not found` æˆ– `401 Unauthorized`

**åŸå› **: æ— æ³•è®¿é—® HuggingFace æ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**:
1. ç™»å½• HuggingFace: `huggingface-cli login`
2. æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œä¿®æ”¹æ¨¡å‹è·¯å¾„
3. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œè®¿é—®æƒé™

## ğŸ“ æœ€å°åŒ–ç¤ºä¾‹

å¦‚æœä½ æƒ³å¿«é€Ÿæµ‹è¯•ä»£ç æ¡†æ¶ï¼ˆä¸ä½¿ç”¨çœŸå®çš„ LLMï¼‰ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼š

```python
# åœ¨ src/model/fusion_llm.py ä¸­
def _load_llm(self, device: str) -> None:
    """æœ€å°åŒ–æµ‹è¯•ç‰ˆæœ¬ - ä»…ç”¨äºä»£ç æ¡†æ¶æµ‹è¯•"""
    import torch.nn as nn
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½ç¬¦æ¨¡å‹ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
    class DummyLLM(nn.Module):
        def __init__(self, vocab_size=32000, hidden_size=4096):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, hidden_size)
            self.layers = nn.ModuleList([
                nn.TransformerEncoderLayer(hidden_size, nhead=8, batch_first=True)
                for _ in range(2)
            ])
        
        def forward(self, input_ids, attention_mask=None, **kwargs):
            x = self.embedding(input_ids)
            for layer in self.layers:
                x = layer(x)
            return type('Output', (), {
                'logits': nn.Linear(x.shape[-1], vocab_size)(x),
                'last_hidden_state': x
            })()
    
    self.llm = DummyLLM()
    self.tokenizer = None  # éœ€è¦å®ç°ä¸€ä¸ªç®€å•çš„ tokenizer
    
    print("âš ï¸ Warning: Using dummy LLM for testing only!")
```

**æ³¨æ„**: è¿™åªæ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œä¸èƒ½ç”¨äºå®é™…è®­ç»ƒã€‚çœŸå®è®­ç»ƒéœ€è¦åŠ è½½é¢„è®­ç»ƒçš„ LLMã€‚
