# å¿«é€Ÿæµ‹è¯•æŒ‡å—ï¼ˆRTX 3060 12GBï¼‰

æœ¬æŒ‡å—å¸®åŠ©ä½ åœ¨ RTX 3060 12GB æ˜¾å¡ä¸Šå¿«é€Ÿè®¾ç½®å’Œæµ‹è¯•æ¨¡å‹ã€‚

## ğŸ“‹ æ¨èæ¨¡å‹

å¯¹äº RTX 3060 12GBï¼Œæ¨èä»¥ä¸‹å°æ¨¡å‹ï¼ˆæŒ‰æ¨èé¡ºåºï¼‰ï¼š

1. **Qwen2.5-1.5B-Instruct** (æ¨è â­)
   - æ¨¡å‹ï¼š`Qwen/Qwen2.5-1.5B-Instruct`
   - å‚æ•°é‡ï¼š1.5B
   - æ˜¾å­˜éœ€æ±‚ï¼š~3GB (FP16)
   - æ”¯æŒä¸­æ–‡ï¼Œæ€§èƒ½ä¼˜ç§€

2. **TinyLlama-1.1B**
   - æ¨¡å‹ï¼š`TinyLlama/TinyLlama-1.1B-Chat-v1.0`
   - å‚æ•°é‡ï¼š1.1B
   - æ˜¾å­˜éœ€æ±‚ï¼š~2.5GB (FP16)
   - è½»é‡çº§ï¼Œé€‚åˆæµ‹è¯•

3. **ChatGLM3-6B** (å¦‚æœå†…å­˜è¶³å¤Ÿ)
   - æ¨¡å‹ï¼š`THUDM/chatglm3-6b`
   - å‚æ•°é‡ï¼š6B
   - æ˜¾å­˜éœ€æ±‚ï¼š~12GB (FP16)ï¼Œéœ€è¦ä½¿ç”¨é‡åŒ–
   - æ”¯æŒä¸­æ–‡

## ğŸ”§ å®‰è£…æ­¥éª¤

### æ­¥éª¤ 1: å®‰è£…ä¾èµ–

```bash
# å®‰è£… transformers å’Œ peft
pip install transformers>=4.35.0 peft>=0.6.0

# æˆ–è€…å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt
```

### æ­¥éª¤ 2: ç™»å½• HuggingFaceï¼ˆå¯é€‰ä½†æ¨èï¼‰

```bash
# å®‰è£… huggingface-hub
pip install huggingface-hub

# ç™»å½•ï¼ˆéœ€è¦ HuggingFace è´¦å·ï¼‰
huggingface-cli login

# æˆ–è€…è®¾ç½® token
# export HF_TOKEN=your_token_here
```

**æ³¨æ„**ï¼š
- æŸäº›æ¨¡å‹ï¼ˆå¦‚ LLaMAï¼‰éœ€è¦ç”³è¯·è®¿é—®æƒé™
- Qwen å’Œ TinyLlama é€šå¸¸ä¸éœ€è¦ç‰¹æ®Šæƒé™
- å¦‚æœæ²¡æœ‰è´¦å·ï¼Œå¯ä»¥æ³¨å†Œï¼šhttps://huggingface.co/join

## ğŸ“ å®ç° LLM åŠ è½½ä»£ç 

ç¼–è¾‘ `src/model/fusion_llm.py`ï¼Œæ‰¾åˆ° `_load_llm` å’Œ `_apply_lora` æ–¹æ³•ï¼Œæ›¿æ¢ä¸ºä»¥ä¸‹ä»£ç ï¼š

### æ–¹æ³• 1: ä½¿ç”¨ Qwen2.5-1.5Bï¼ˆæ¨èï¼‰

```python
def _load_llm(self, device: str) -> None:
    """åŠ è½½é¢„è®­ç»ƒ LLM"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.config.llm_model_name}")
    print(f"è®¾å¤‡: {device}")
    
    # è®¾ç½®æ•°æ®ç±»å‹
    torch_dtype = torch.float16 if device == "cuda" else torch.float32
    
    # åŠ è½½æ¨¡å‹
    self.llm = AutoModelForCausalLM.from_pretrained(
        self.config.llm_model_name,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # åŠ è½½åˆ†è¯å™¨
    self.tokenizer = AutoTokenizer.from_pretrained(
        self.config.llm_model_name,
        trust_remote_code=True
    )
    
    # è®¾ç½® pad token
    if self.tokenizer.pad_token is None:
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
    
    # ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆå¦‚æœä½¿ç”¨ CPUï¼‰
    if device == "cpu":
        self.llm = self.llm.to(device)
    
    print("æ¨¡å‹åŠ è½½å®Œæˆ!")

def _apply_lora(self) -> None:
    """åº”ç”¨ LoRA é€‚é…å™¨"""
    from peft import LoraConfig, get_peft_model, TaskType
    
    print("æ­£åœ¨åº”ç”¨ LoRA é€‚é…å™¨...")
    
    # ç¡®å®šç›®æ ‡æ¨¡å—ï¼ˆæ ¹æ®æ¨¡å‹æ¶æ„è°ƒæ•´ï¼‰
    model_name_lower = self.config.llm_model_name.lower()
    if "qwen" in model_name_lower:
        # Qwen æ¨¡å‹çš„æ¨¡å—åç§°
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif "llama" in model_name_lower or "tinyllama" in model_name_lower:
        # LLaMA æ¨¡å‹çš„æ¨¡å—åç§°
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif "chatglm" in model_name_lower:
        # ChatGLM æ¨¡å‹çš„æ¨¡å—åç§°
        target_modules = ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
    else:
        # é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å—
        target_modules = self.config.lora_target_modules
    
    lora_config = LoraConfig(
        r=self.config.lora_r,
        lora_alpha=self.config.lora_alpha,
        lora_dropout=self.config.lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    self.llm = get_peft_model(self.llm, lora_config)
    self.llm.print_trainable_parameters()
    print("LoRA é€‚é…å™¨åº”ç”¨å®Œæˆ!")
```

## âš™ï¸ æ›´æ–°é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/default.yaml`ï¼Œå°†æ¨¡å‹åç§°æ”¹ä¸ºï¼š

```yaml
model:
  llm:
    model_name: "Qwen/Qwen2.5-1.5B-Instruct"  # æ¨èç”¨äºæµ‹è¯•
    # æˆ–è€…ä½¿ç”¨: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
```

## ğŸš€ æµ‹è¯•æ­¥éª¤

### æ­¥éª¤ 1: æµ‹è¯•æ¨¡å‹åŠ è½½

åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬ `scripts/test_model_loading.py`ï¼š

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""æµ‹è¯•æ¨¡å‹åŠ è½½"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.model.fusion_llm import ResilienceLLM, ModelConfig

def test_model_loading():
    config = ModelConfig(
        llm_model_name="Qwen/Qwen2.5-1.5B-Instruct",
        use_lora=True,
        lora_r=8
    )
    
    print("åˆ›å»ºæ¨¡å‹...")
    model = ResilienceLLM(config)
    
    print("åˆå§‹åŒ–æ¨¡å‹...")
    try:
        model.initialize(device="cuda")
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in model.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_model_loading()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python scripts/test_model_loading.py
```

### æ­¥éª¤ 2: å¼€å§‹è®­ç»ƒ

å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼š

```bash
python scripts/train.py \
    --train_data data/fine_tuning/combined/train.json \
    --eval_data data/fine_tuning/combined/eval.json \
    --output_dir outputs/test_model \
    --phase 1 \
    --epochs 1 \
    --batch_size 2  # å° batch size é€‚åˆæµ‹è¯•
```

## ğŸ” å¸¸è§é—®é¢˜

### Q1: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å° batch size: `--batch_size 1`
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ TinyLlamaï¼‰
3. ä½¿ç”¨é‡åŒ–ï¼ˆéœ€è¦ bitsandbytesï¼‰ï¼š
   ```python
   from transformers import BitsAndBytesConfig
   quantization_config = BitsAndBytesConfig(
       load_in_4bit=True,
       bnb_4bit_compute_dtype=torch.float16
   )
   ```

### Q2: æ¨¡å‹ä¸‹è½½æ…¢

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨é•œåƒç«™ç‚¹ï¼ˆä¸­å›½ç”¨æˆ·ï¼‰
2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```
3. æˆ–è€…æ‰‹åŠ¨ä¸‹è½½åˆ°æœ¬åœ°

### Q3: æ‰¾ä¸åˆ°æ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤ HuggingFace è®¿é—®æƒé™
3. æ£€æŸ¥ç½‘ç»œè¿æ¥
4. å°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ TinyLlamaï¼‰

## ğŸ“Š æ˜¾å­˜ä½¿ç”¨ä¼°ç®—

å¯¹äº RTX 3060 12GBï¼š

| æ¨¡å‹ | å‚æ•°é‡ | FP16 æ˜¾å­˜ | æ¨è batch size |
|------|--------|-----------|----------------|
| Qwen2.5-1.5B | 1.5B | ~3GB | 4-8 |
| TinyLlama-1.1B | 1.1B | ~2.5GB | 4-8 |
| ChatGLM3-6B (4bit) | 6B | ~4GB | 2-4 |

## âœ… éªŒè¯æ¸…å•

- [ ] å®‰è£… transformers å’Œ peft
- [ ] ç™»å½• HuggingFaceï¼ˆå¦‚æœéœ€è¦ï¼‰
- [ ] å®ç° `_load_llm` æ–¹æ³•
- [ ] å®ç° `_apply_lora` æ–¹æ³•
- [ ] æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹åç§°
- [ ] æµ‹è¯•æ¨¡å‹åŠ è½½æˆåŠŸ
- [ ] æ£€æŸ¥å¯è®­ç»ƒå‚æ•°æ•°é‡
- [ ] å¼€å§‹å°è§„æ¨¡è®­ç»ƒæµ‹è¯•
