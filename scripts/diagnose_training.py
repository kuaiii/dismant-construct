#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®­ç»ƒé—®é¢˜è¯Šæ–­è„šæœ¬
ç”¨äºæ’æŸ¥ NaN/Inf é—®é¢˜çš„æ ¹æº
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import json
import numpy as np


def check_data_quality(data_path: str):
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    print("=" * 60)
    print("1. æ£€æŸ¥æ•°æ®è´¨é‡")
    print("=" * 60)
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"æ ·æœ¬æ•°é‡: {len(data)}")
    
    issues = []
    for i, sample in enumerate(data):
        aux_labels = sample.get("auxiliary_labels", {})
        
        # æ£€æŸ¥ auxiliary_labels
        for op_id, value in aux_labels.items():
            if value != value:  # NaN check
                issues.append(f"æ ·æœ¬ {i} ({sample.get('id', 'unknown')}): {op_id} å€¼ä¸º NaN")
            if abs(value) == float('inf'):
                issues.append(f"æ ·æœ¬ {i} ({sample.get('id', 'unknown')}): {op_id} å€¼ä¸º Inf")
            if abs(value) > 100:
                issues.append(f"æ ·æœ¬ {i} ({sample.get('id', 'unknown')}): {op_id} å€¼å¼‚å¸¸å¤§: {value}")
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        convs = sample.get("conversations", [])
        total_len = sum(len(c.get("value", "")) for c in convs)
        if total_len > 10000:
            issues.append(f"æ ·æœ¬ {i}: æ–‡æœ¬æ€»é•¿åº¦è¿‡é•¿: {total_len}")
    
    if issues:
        print(f"\nâš ï¸ å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
        for issue in issues[:20]:  # åªæ˜¾ç¤ºå‰ 20 ä¸ª
            print(f"  - {issue}")
        if len(issues) > 20:
            print(f"  ... è¿˜æœ‰ {len(issues) - 20} ä¸ªé—®é¢˜")
    else:
        print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°å¼‚å¸¸å€¼")
    
    # ç»Ÿè®¡ auxiliary_labels åˆ†å¸ƒ
    all_values = []
    for sample in data:
        all_values.extend(sample.get("auxiliary_labels", {}).values())
    
    if all_values:
        print(f"\nauxiliary_labels ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {min(all_values):.6f}")
        print(f"  æœ€å¤§å€¼: {max(all_values):.6f}")
        print(f"  å‡å€¼: {np.mean(all_values):.6f}")
        print(f"  æ ‡å‡†å·®: {np.std(all_values):.6f}")
        print(f"  é›¶å€¼æ¯”ä¾‹: {sum(1 for v in all_values if v == 0) / len(all_values) * 100:.1f}%")
    
    return len(issues) == 0


def check_model_forward(model_name: str = "Qwen/Qwen2.5-1.5B-Instruct"):
    """æ£€æŸ¥æ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\n" + "=" * 60)
    print("2. æ£€æŸ¥æ¨¡å‹å‰å‘ä¼ æ’­")
    print("=" * 60)
    
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æµ‹è¯•ä¸åŒç²¾åº¦
    for dtype_name, dtype in [("FP32", torch.float32), ("FP16", torch.float16)]:
        print(f"\næµ‹è¯• {dtype_name} ç²¾åº¦:")
        
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=dtype,
                trust_remote_code=True,
                device_map="auto"
            )
            model.eval()
            
            # ç®€å•æµ‹è¯•
            test_text = "Hello, how are you?"
            inputs = tokenizer(test_text, return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
            
            has_nan = torch.isnan(logits).any().item()
            has_inf = torch.isinf(logits).any().item()
            
            if has_nan or has_inf:
                print(f"  âŒ {dtype_name} è¾“å‡ºåŒ…å« NaN/Inf!")
            else:
                print(f"  âœ… {dtype_name} è¾“å‡ºæ­£å¸¸")
                print(f"     logits èŒƒå›´: [{logits.min().item():.4f}, {logits.max().item():.4f}]")
            
            del model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"  âŒ {dtype_name} æµ‹è¯•å¤±è´¥: {e}")
    
    return True


def check_training_step():
    """æ£€æŸ¥å•æ­¥è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("3. æ£€æŸ¥å•æ­¥è®­ç»ƒ (ç¦ç”¨ FP16)")
    print("=" * 60)
    
    import yaml
    from src.model.fusion_llm import ResilienceLLM, ModelConfig
    from src.data.dataset import create_dataloader
    
    # åŠ è½½é…ç½®
    with open("configs/default.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºæ¨¡å‹
    model_config = ModelConfig(
        llm_model_name=config['model']['llm']['model_name'],
        use_lora=config['model']['lora']['enabled'],
        lora_r=config['model']['lora']['r'],
        lora_alpha=config['model']['lora']['alpha'],
        lora_dropout=config['model']['lora']['dropout'],
    )
    
    print("åˆå§‹åŒ–æ¨¡å‹...")
    model = ResilienceLLM(model_config)
    model.initialize(device="cuda")
    model.train()
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    train_loader = create_dataloader(
        data_path="data/fine_tuning/combined/train.json",
        tokenizer=model.tokenizer,
        batch_size=1,
        shuffle=False,
        max_length=config['data']['loading']['max_length']
    )
    
    # æµ‹è¯•å‡ ä¸ª batch
    print("\næµ‹è¯•å‰ 10 ä¸ª batch:")
    for i, batch in enumerate(train_loader):
        if i >= 10:
            break
        
        input_ids = batch["input_ids"].to("cuda")
        attention_mask = batch["attention_mask"].to("cuda")
        
        # ç¦ç”¨ AMPï¼Œä½¿ç”¨çº¯ FP32
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_scores=True
            )
        
        logits = outputs.get("logits")
        if logits is not None:
            has_nan = torch.isnan(logits).any().item()
            has_inf = torch.isinf(logits).any().item()
            
            if has_nan or has_inf:
                print(f"  Batch {i}: âŒ logits åŒ…å« NaN/Inf")
                print(f"    æ ·æœ¬ ID: {batch.get('sample_ids', ['unknown'])[0]}")
                print(f"    è¾“å…¥é•¿åº¦: {attention_mask.sum().item()}")
            else:
                print(f"  Batch {i}: âœ… æ­£å¸¸, logits èŒƒå›´: [{logits.min().item():.2f}, {logits.max().item():.2f}]")
        else:
            print(f"  Batch {i}: logits ä¸º None")
    
    return True


def suggest_fixes():
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print("\n" + "=" * 60)
    print("4. å»ºè®®çš„ä¿®å¤æ–¹æ¡ˆ")
    print("=" * 60)
    
    print("""
æ ¹æ®è¯Šæ–­ç»“æœï¼Œå»ºè®®å°è¯•ä»¥ä¸‹ä¿®å¤æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰:

ã€æ–¹æ¡ˆ 1ã€‘ç¦ç”¨ FP16 æ··åˆç²¾åº¦è®­ç»ƒ
  ä¿®æ”¹ configs/default.yaml:
  training:
    fp16: false
    bf16: false

ã€æ–¹æ¡ˆ 2ã€‘é™ä½å­¦ä¹ ç‡
  python scripts/train.py --lr 1e-5

ã€æ–¹æ¡ˆ 3ã€‘ä½¿ç”¨ BF16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
  training:
    fp16: false
    bf16: true

ã€æ–¹æ¡ˆ 4ã€‘å‡å°æ¢¯åº¦è£å‰ªé˜ˆå€¼
  optimizer:
    max_grad_norm: 0.5

ã€æ–¹æ¡ˆ 5ã€‘æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®é›†
  - ç§»é™¤åŒ…å«å¼‚å¸¸å€¼çš„æ ·æœ¬
  - å½’ä¸€åŒ– auxiliary_labels
""")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="è®­ç»ƒé—®é¢˜è¯Šæ–­")
    parser.add_argument("--data", type=str, default="data/fine_tuning/combined/train.json")
    parser.add_argument("--skip-model", action="store_true", help="è·³è¿‡æ¨¡å‹æµ‹è¯•")
    parser.add_argument("--skip-training", action="store_true", help="è·³è¿‡è®­ç»ƒæµ‹è¯•")
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹è¯Šæ–­è®­ç»ƒé—®é¢˜...\n")
    
    # 1. æ£€æŸ¥æ•°æ®
    data_ok = check_data_quality(args.data)
    
    # 2. æ£€æŸ¥æ¨¡å‹
    if not args.skip_model:
        check_model_forward()
    
    # 3. æ£€æŸ¥è®­ç»ƒ
    if not args.skip_training:
        try:
            check_training_step()
        except Exception as e:
            print(f"è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # 4. å»ºè®®ä¿®å¤
    suggest_fixes()
    
    print("\n" + "=" * 60)
    print("è¯Šæ–­å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
